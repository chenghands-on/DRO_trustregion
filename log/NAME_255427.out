Sender: LSF System <lsfadmin@gauss01>
Subject: Job 255427: <DROSGD> in cluster <cjdx.cluster> Done

Job <DROSGD> was submitted from host <alpha02> by user <xiechenghan> in cluster <cjdx.cluster> at Thu May 18 19:14:45 2023
Job was executed on host(s) <gauss01>, in queue <gauss>, as user <xiechenghan> in cluster <cjdx.cluster> at Thu May 18 19:14:46 2023
</nfsshare/home/xiechenghan> was used as the home directory.
</nfsshare/home/xiechenghan/DRO_trustregion> was used as the working directory.
Started at Thu May 18 19:14:46 2023
Terminated at Fri May 19 02:09:14 2023
Results reported at Fri May 19 02:09:14 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#/bin/bash
#BSUB -J DROSGD
#BSUB -e /nfsshare/home/xiechenghan/DRO_trustregion/log/NAME_%J.err
#BSUB -o /nfsshare/home/xiechenghan/DRO_trustregion/log/NAME_%J.out
#BSUB -n 1
#BSUB -q gauss
#BSUB -gpu "num=1:mode=exclusive_process"
python my_train.py --optim sgd
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   431194.69 sec.
    Max Memory :                                 2323 MB
    Average Memory :                             2246.59 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                154
    Run time :                                   24868 sec.
    Turnaround time :                            24869 sec.

The output (if any) follows:

{
  "eta": 0.08,
  "beta1": 50,
  "beta2": 30,
  "zeta1": 0.25,
  "zeta2": 0.75
}
{
  "qp_rate": 0.3,
  "qp_freq": 1,
  "decay_mode": 1,
  "decay_window": 8000,
  "decay_step": 500.0,
  "decay_radius_step": 0.2,
  "decay_sin_rel_max": 100000000.0,
  "decay_sin_min_scope": 0,
  "decay_sin_max_scope": 37600
}
Using [cuda] for the work
train from scratch!
-----------------------epoch 1-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0030406 MAE:16.93282 [32/165515]
training || loss:0.0011890 MAE:11.67693 [3232/165515]
training || loss:0.0021915 MAE:16.76875 [6432/165515]
training || loss:0.0015163 MAE:14.10738 [9632/165515]
training || loss:0.0030497 MAE:18.29958 [12832/165515]
training || loss:0.0025560 MAE:16.69166 [16032/165515]
training || loss:0.0009812 MAE:10.34240 [19232/165515]
training || loss:0.0008113 MAE:10.09802 [22432/165515]
training || loss:0.0015463 MAE:13.41710 [25632/165515]
training || loss:0.0011391 MAE:12.35530 [28832/165515]
training || loss:0.0014429 MAE:14.05107 [32032/165515]
training || loss:0.0011551 MAE:9.86364 [35232/165515]
training || loss:0.0014141 MAE:12.75673 [38432/165515]
training || loss:0.0014745 MAE:12.79280 [41632/165515]
training || loss:0.0011082 MAE:10.18378 [44832/165515]
training || loss:0.0013236 MAE:12.09145 [48032/165515]
training || loss:0.0011406 MAE:13.06316 [51232/165515]
training || loss:0.0010319 MAE:10.62135 [54432/165515]
training || loss:0.0010555 MAE:10.60981 [57632/165515]
training || loss:0.0006248 MAE:7.96698 [60832/165515]
training || loss:0.0006713 MAE:7.09737 [64032/165515]
training || loss:0.0009839 MAE:10.94387 [67232/165515]
training || loss:0.0007928 MAE:10.10609 [70432/165515]
training || loss:0.0010270 MAE:10.55911 [73632/165515]
training || loss:0.0007892 MAE:9.98749 [76832/165515]
training || loss:0.0007502 MAE:8.26674 [80032/165515]
training || loss:0.0006662 MAE:9.37356 [83232/165515]
training || loss:0.0009744 MAE:11.26791 [86432/165515]
training || loss:0.0005227 MAE:8.97343 [89632/165515]
training || loss:0.0006325 MAE:8.36880 [92832/165515]
training || loss:0.0010203 MAE:11.12657 [96032/165515]
training || loss:0.0009000 MAE:9.75364 [99232/165515]
training || loss:0.0006541 MAE:9.15688 [102432/165515]
training || loss:0.0009149 MAE:9.76769 [105632/165515]
training || loss:0.0005553 MAE:8.11063 [108832/165515]
training || loss:0.0007974 MAE:8.50816 [112032/165515]
training || loss:0.0006904 MAE:7.13937 [115232/165515]
training || loss:0.0008378 MAE:9.20527 [118432/165515]
training || loss:0.0006449 MAE:9.69752 [121632/165515]
training || loss:0.0010458 MAE:10.23913 [124832/165515]
training || loss:0.0007238 MAE:9.56396 [128032/165515]
training || loss:0.0008167 MAE:9.84007 [131232/165515]
training || loss:0.0005050 MAE:6.71690 [134432/165515]
training || loss:0.0009825 MAE:9.08030 [137632/165515]
training || loss:0.0006384 MAE:7.07754 [140832/165515]
training || loss:0.0006760 MAE:9.94967 [144032/165515]
training || loss:0.0005837 MAE:7.43090 [147232/165515]
training || loss:0.0006659 MAE:8.20210 [150432/165515]
training || loss:0.0007591 MAE:8.59186 [153632/165515]
training || loss:0.0005187 MAE:7.44290 [156832/165515]
training || loss:0.0005736 MAE:7.55203 [160032/165515]
training || loss:0.0004223 MAE:6.26322 [163232/165515]
validate|| MAE:2.28242
-----------------------epoch 2-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0005493 MAE:8.62691 [32/165515]
training || loss:0.0005330 MAE:7.84281 [3232/165515]
training || loss:0.0005195 MAE:7.78738 [6432/165515]
training || loss:0.0007500 MAE:9.63768 [9632/165515]
training || loss:0.0004356 MAE:6.86963 [12832/165515]
training || loss:0.0006483 MAE:8.77860 [16032/165515]
training || loss:0.0003904 MAE:6.86131 [19232/165515]
training || loss:0.0005454 MAE:9.12451 [22432/165515]
training || loss:0.0006379 MAE:9.45955 [25632/165515]
training || loss:0.0006636 MAE:9.50129 [28832/165515]
training || loss:0.0005574 MAE:7.52187 [32032/165515]
training || loss:0.0004787 MAE:7.57470 [35232/165515]
training || loss:0.0005298 MAE:6.80788 [38432/165515]
training || loss:0.0005882 MAE:8.55749 [41632/165515]
training || loss:0.0004197 MAE:7.75316 [44832/165515]
training || loss:0.0006706 MAE:8.89957 [48032/165515]
training || loss:0.0005496 MAE:8.60801 [51232/165515]
training || loss:0.0004898 MAE:6.09488 [54432/165515]
training || loss:0.0006145 MAE:7.31039 [57632/165515]
training || loss:0.0007922 MAE:9.98587 [60832/165515]
training || loss:0.0005868 MAE:9.72689 [64032/165515]
training || loss:0.0005322 MAE:8.18546 [67232/165515]
training || loss:0.0005761 MAE:7.72203 [70432/165515]
training || loss:0.0004368 MAE:7.22095 [73632/165515]
training || loss:0.0004858 MAE:7.66593 [76832/165515]
training || loss:0.0004013 MAE:7.30677 [80032/165515]
training || loss:0.0004380 MAE:7.46033 [83232/165515]
training || loss:0.0004863 MAE:7.28431 [86432/165515]
training || loss:0.0006031 MAE:9.43729 [89632/165515]
training || loss:0.0004159 MAE:7.45508 [92832/165515]
training || loss:0.0004984 MAE:6.81140 [96032/165515]
training || loss:0.0004387 MAE:8.47685 [99232/165515]
training || loss:0.0006548 MAE:8.32023 [102432/165515]
training || loss:0.0005590 MAE:8.32930 [105632/165515]
training || loss:0.0004102 MAE:6.31440 [108832/165515]
training || loss:0.0004291 MAE:6.83057 [112032/165515]
training || loss:0.0005395 MAE:7.92115 [115232/165515]
training || loss:0.0005024 MAE:7.25784 [118432/165515]
training || loss:0.0005779 MAE:8.59110 [121632/165515]
training || loss:0.0004525 MAE:7.14831 [124832/165515]
training || loss:0.0004577 MAE:5.83631 [128032/165515]
training || loss:0.0005410 MAE:9.11790 [131232/165515]
training || loss:0.0005078 MAE:8.08182 [134432/165515]
training || loss:0.0005913 MAE:6.97502 [137632/165515]
training || loss:0.0003513 MAE:6.36360 [140832/165515]
training || loss:0.0003976 MAE:6.73612 [144032/165515]
training || loss:0.0004653 MAE:7.36006 [147232/165515]
training || loss:0.0004197 MAE:7.36595 [150432/165515]
training || loss:0.0003923 MAE:7.48652 [153632/165515]
training || loss:0.0004645 MAE:7.59792 [156832/165515]
training || loss:0.0004838 MAE:7.65951 [160032/165515]
training || loss:0.0003748 MAE:6.09022 [163232/165515]
validate|| MAE:2.28280
-----------------------epoch 3-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0004594 MAE:7.34825 [32/165515]
training || loss:0.0004326 MAE:6.59784 [3232/165515]
training || loss:0.0003049 MAE:4.79632 [6432/165515]
training || loss:0.0002836 MAE:4.89138 [9632/165515]
training || loss:0.0002968 MAE:5.25542 [12832/165515]
training || loss:0.0003444 MAE:7.03188 [16032/165515]
training || loss:0.0004445 MAE:7.68746 [19232/165515]
training || loss:0.0005578 MAE:8.89011 [22432/165515]
training || loss:0.0004096 MAE:8.62039 [25632/165515]
training || loss:0.0004680 MAE:7.45083 [28832/165515]
training || loss:0.0004487 MAE:6.68895 [32032/165515]
training || loss:0.0004047 MAE:5.40409 [35232/165515]
training || loss:0.0003679 MAE:6.96817 [38432/165515]
training || loss:0.0003768 MAE:5.72479 [41632/165515]
training || loss:0.0003431 MAE:6.58222 [44832/165515]
training || loss:0.0005314 MAE:7.41435 [48032/165515]
training || loss:0.0004019 MAE:7.02364 [51232/165515]
training || loss:0.0003091 MAE:6.39290 [54432/165515]
training || loss:0.0003595 MAE:5.78631 [57632/165515]
training || loss:0.0003055 MAE:6.84293 [60832/165515]
training || loss:0.0003212 MAE:5.80611 [64032/165515]
training || loss:0.0005170 MAE:7.92659 [67232/165515]
training || loss:0.0003805 MAE:6.19872 [70432/165515]
training || loss:0.0004987 MAE:7.48447 [73632/165515]
training || loss:0.0006312 MAE:8.31571 [76832/165515]
training || loss:0.0004977 MAE:7.68750 [80032/165515]
training || loss:0.0003554 MAE:5.41852 [83232/165515]
training || loss:0.0003379 MAE:5.90447 [86432/165515]
training || loss:0.0003103 MAE:5.94115 [89632/165515]
training || loss:0.0003928 MAE:6.77300 [92832/165515]
training || loss:0.0004385 MAE:6.68160 [96032/165515]
training || loss:0.0005196 MAE:7.40383 [99232/165515]
training || loss:0.0003336 MAE:6.24100 [102432/165515]
training || loss:0.0002782 MAE:4.82819 [105632/165515]
training || loss:0.0003324 MAE:6.44838 [108832/165515]
training || loss:0.0004439 MAE:6.44282 [112032/165515]
training || loss:0.0003932 MAE:6.70020 [115232/165515]
training || loss:0.0004328 MAE:6.17472 [118432/165515]
training || loss:0.0002240 MAE:4.33901 [121632/165515]
training || loss:0.0003327 MAE:6.38451 [124832/165515]
training || loss:0.0002837 MAE:5.50105 [128032/165515]
training || loss:0.0007222 MAE:6.78073 [131232/165515]
training || loss:0.0004280 MAE:7.77679 [134432/165515]
training || loss:0.0003546 MAE:5.88024 [137632/165515]
training || loss:0.0002913 MAE:6.07101 [140832/165515]
training || loss:0.0004708 MAE:6.79483 [144032/165515]
training || loss:0.0004164 MAE:5.89923 [147232/165515]
training || loss:0.0003977 MAE:5.79661 [150432/165515]
training || loss:0.0003009 MAE:5.72058 [153632/165515]
training || loss:0.0005278 MAE:8.05334 [156832/165515]
training || loss:0.0003769 MAE:5.81902 [160032/165515]
training || loss:0.0006741 MAE:7.64612 [163232/165515]
validate|| MAE:2.28818
-----------------------epoch 4-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0003287 MAE:6.42536 [32/165515]
training || loss:0.0003381 MAE:7.41011 [3232/165515]
training || loss:0.0003484 MAE:6.76501 [6432/165515]
training || loss:0.0003212 MAE:6.19151 [9632/165515]
training || loss:0.0002974 MAE:5.83307 [12832/165515]
training || loss:0.0003437 MAE:6.09385 [16032/165515]
training || loss:0.0003683 MAE:7.22527 [19232/165515]
training || loss:0.0003102 MAE:5.38349 [22432/165515]
training || loss:0.0002871 MAE:6.04380 [25632/165515]
training || loss:0.0002940 MAE:6.01929 [28832/165515]
training || loss:0.0004667 MAE:8.17162 [32032/165515]
training || loss:0.0002680 MAE:5.14163 [35232/165515]
training || loss:0.0003283 MAE:6.18217 [38432/165515]
training || loss:0.0003148 MAE:6.61275 [41632/165515]
training || loss:0.0002649 MAE:5.12969 [44832/165515]
training || loss:0.0005420 MAE:6.49888 [48032/165515]
training || loss:0.0003652 MAE:6.95730 [51232/165515]
training || loss:0.0004258 MAE:7.21830 [54432/165515]
training || loss:0.0003533 MAE:5.85753 [57632/165515]
training || loss:0.0003180 MAE:6.55827 [60832/165515]
training || loss:0.0003828 MAE:6.94386 [64032/165515]
training || loss:0.0002695 MAE:4.32514 [67232/165515]
training || loss:0.0003506 MAE:5.79898 [70432/165515]
training || loss:0.0004146 MAE:6.86382 [73632/165515]
training || loss:0.0002501 MAE:4.64081 [76832/165515]
training || loss:0.0002985 MAE:5.49003 [80032/165515]
training || loss:0.0003635 MAE:5.49548 [83232/165515]
training || loss:0.0002460 MAE:5.11182 [86432/165515]
training || loss:0.0002816 MAE:5.56001 [89632/165515]
training || loss:0.0002595 MAE:6.14168 [92832/165515]
training || loss:0.0003606 MAE:6.65239 [96032/165515]
training || loss:0.0003746 MAE:7.24321 [99232/165515]
training || loss:0.0003168 MAE:6.05265 [102432/165515]
training || loss:0.0003450 MAE:6.65291 [105632/165515]
training || loss:0.0003201 MAE:5.91431 [108832/165515]
training || loss:0.0003081 MAE:6.03713 [112032/165515]
training || loss:0.0002801 MAE:4.62382 [115232/165515]
training || loss:0.0002926 MAE:6.41711 [118432/165515]
training || loss:0.0002323 MAE:5.08984 [121632/165515]
training || loss:0.0002676 MAE:5.39515 [124832/165515]
training || loss:0.0003592 MAE:6.79927 [128032/165515]
training || loss:0.0003899 MAE:6.59591 [131232/165515]
training || loss:0.0004230 MAE:7.62594 [134432/165515]
training || loss:0.0002658 MAE:5.05006 [137632/165515]
training || loss:0.0002652 MAE:6.09075 [140832/165515]
training || loss:0.0002326 MAE:3.75229 [144032/165515]
training || loss:0.0002411 MAE:4.21781 [147232/165515]
training || loss:0.0003203 MAE:5.67216 [150432/165515]
training || loss:0.0002774 MAE:6.10539 [153632/165515]
training || loss:0.0002755 MAE:5.46568 [156832/165515]
training || loss:0.0002927 MAE:5.38116 [160032/165515]
training || loss:0.0002516 MAE:5.25282 [163232/165515]
validate|| MAE:2.29323
-----------------------epoch 5-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0002332 MAE:4.47370 [32/165515]
training || loss:0.0004047 MAE:6.50524 [3232/165515]
training || loss:0.0002828 MAE:5.72425 [6432/165515]
training || loss:0.0001971 MAE:4.72082 [9632/165515]
training || loss:0.0003477 MAE:5.97011 [12832/165515]
training || loss:0.0003274 MAE:6.90818 [16032/165515]
training || loss:0.0002377 MAE:5.09451 [19232/165515]
training || loss:0.0003786 MAE:7.89483 [22432/165515]
training || loss:0.0004562 MAE:7.18152 [25632/165515]
training || loss:0.0002766 MAE:6.13618 [28832/165515]
training || loss:0.0002605 MAE:4.90111 [32032/165515]
training || loss:0.0002913 MAE:6.33975 [35232/165515]
training || loss:0.0002368 MAE:5.96085 [38432/165515]
training || loss:0.0003525 MAE:6.09566 [41632/165515]
training || loss:0.0003038 MAE:5.56130 [44832/165515]
training || loss:0.0003256 MAE:6.45496 [48032/165515]
training || loss:0.0003181 MAE:6.81398 [51232/165515]
training || loss:0.0001969 MAE:5.12530 [54432/165515]
training || loss:0.0003095 MAE:6.59931 [57632/165515]
training || loss:0.0003210 MAE:6.84682 [60832/165515]
training || loss:0.0003686 MAE:7.30922 [64032/165515]
training || loss:0.0002754 MAE:5.29162 [67232/165515]
training || loss:0.0002607 MAE:5.49968 [70432/165515]
training || loss:0.0002720 MAE:6.14048 [73632/165515]
training || loss:0.0003158 MAE:5.87000 [76832/165515]
training || loss:0.0002843 MAE:6.51484 [80032/165515]
training || loss:0.0002788 MAE:5.64636 [83232/165515]
training || loss:0.0003015 MAE:6.04357 [86432/165515]
training || loss:0.0004038 MAE:6.69533 [89632/165515]
training || loss:0.0002188 MAE:4.54994 [92832/165515]
training || loss:0.0003300 MAE:7.19847 [96032/165515]
training || loss:0.0002724 MAE:6.13157 [99232/165515]
training || loss:0.0002251 MAE:5.45968 [102432/165515]
training || loss:0.0003073 MAE:5.90223 [105632/165515]
training || loss:0.0003547 MAE:5.52091 [108832/165515]
training || loss:0.0003256 MAE:6.30408 [112032/165515]
training || loss:0.0002335 MAE:6.08345 [115232/165515]
training || loss:0.0002932 MAE:5.72693 [118432/165515]
training || loss:0.0002621 MAE:5.40445 [121632/165515]
training || loss:0.0001596 MAE:4.34559 [124832/165515]
training || loss:0.0002730 MAE:4.73799 [128032/165515]
training || loss:0.0001979 MAE:4.82337 [131232/165515]
training || loss:0.0002944 MAE:6.40036 [134432/165515]
training || loss:0.0002806 MAE:6.09790 [137632/165515]
training || loss:0.0003303 MAE:5.86915 [140832/165515]
training || loss:0.0002668 MAE:5.65450 [144032/165515]
training || loss:0.0003570 MAE:6.47663 [147232/165515]
training || loss:0.0003300 MAE:6.26307 [150432/165515]
training || loss:0.0003169 MAE:6.33847 [153632/165515]
training || loss:0.0005834 MAE:6.71655 [156832/165515]
training || loss:0.0002455 MAE:5.44841 [160032/165515]
training || loss:0.0002929 MAE:6.49587 [163232/165515]
validate|| MAE:2.29547
-----------------------epoch 6-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0003411 MAE:6.97101 [32/165515]
training || loss:0.0003456 MAE:6.63393 [3232/165515]
training || loss:0.0002328 MAE:4.83293 [6432/165515]
training || loss:0.0002449 MAE:6.12828 [9632/165515]
training || loss:0.0002126 MAE:5.32405 [12832/165515]
training || loss:0.0002704 MAE:5.98623 [16032/165515]
training || loss:0.0002429 MAE:4.96066 [19232/165515]
training || loss:0.0003047 MAE:6.61695 [22432/165515]
training || loss:0.0004342 MAE:6.36857 [25632/165515]
training || loss:0.0003073 MAE:5.77612 [28832/165515]
training || loss:0.0002972 MAE:6.19892 [32032/165515]
training || loss:0.0002174 MAE:5.87881 [35232/165515]
training || loss:0.0003189 MAE:6.36642 [38432/165515]
training || loss:0.0003494 MAE:6.74750 [41632/165515]
training || loss:0.0001890 MAE:4.38822 [44832/165515]
training || loss:0.0003083 MAE:6.78104 [48032/165515]
training || loss:0.0002179 MAE:5.10819 [51232/165515]
training || loss:0.0006085 MAE:6.52431 [54432/165515]
training || loss:0.0003127 MAE:6.00545 [57632/165515]
training || loss:0.0001965 MAE:5.55041 [60832/165515]
training || loss:0.0002777 MAE:5.56965 [64032/165515]
training || loss:0.0002316 MAE:5.13058 [67232/165515]
training || loss:0.0002600 MAE:5.30348 [70432/165515]
training || loss:0.0002353 MAE:4.47007 [73632/165515]
training || loss:0.0002425 MAE:5.34095 [76832/165515]
training || loss:0.0001946 MAE:4.64902 [80032/165515]
training || loss:0.0003217 MAE:5.69353 [83232/165515]
training || loss:0.0002051 MAE:4.76834 [86432/165515]
training || loss:0.0002755 MAE:5.74140 [89632/165515]
training || loss:0.0002360 MAE:4.80948 [92832/165515]
training || loss:0.0002367 MAE:5.22699 [96032/165515]
training || loss:0.0002329 MAE:4.27054 [99232/165515]
training || loss:0.0003076 MAE:6.20243 [102432/165515]
training || loss:0.0002605 MAE:5.97024 [105632/165515]
training || loss:0.0002705 MAE:5.09817 [108832/165515]
training || loss:0.0001892 MAE:4.80637 [112032/165515]
training || loss:0.0002640 MAE:5.82419 [115232/165515]
training || loss:0.0002817 MAE:5.88807 [118432/165515]
training || loss:0.0001888 MAE:4.61402 [121632/165515]
training || loss:0.0003077 MAE:5.62425 [124832/165515]
training || loss:0.0002926 MAE:5.85767 [128032/165515]
training || loss:0.0003213 MAE:6.18383 [131232/165515]
training || loss:0.0003234 MAE:6.65472 [134432/165515]
training || loss:0.0002193 MAE:5.03036 [137632/165515]
training || loss:0.0003361 MAE:5.75315 [140832/165515]
training || loss:0.0002015 MAE:4.58234 [144032/165515]
training || loss:0.0002457 MAE:6.30319 [147232/165515]
training || loss:0.0001881 MAE:4.12110 [150432/165515]
training || loss:0.0002454 MAE:5.42623 [153632/165515]
training || loss:0.0003201 MAE:6.26212 [156832/165515]
training || loss:0.0001504 MAE:4.05179 [160032/165515]
training || loss:0.0001943 MAE:5.00423 [163232/165515]
validate|| MAE:2.29805
-----------------------epoch 7-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0001838 MAE:4.31262 [32/165515]
training || loss:0.0003761 MAE:7.15577 [3232/165515]
training || loss:0.0002842 MAE:5.68378 [6432/165515]
training || loss:0.0002352 MAE:5.29559 [9632/165515]
training || loss:0.0003891 MAE:7.22890 [12832/165515]
training || loss:0.0002639 MAE:6.04112 [16032/165515]
training || loss:0.0002476 MAE:4.86826 [19232/165515]
training || loss:0.0001805 MAE:4.58107 [22432/165515]
training || loss:0.0002979 MAE:6.13229 [25632/165515]
training || loss:0.0002081 MAE:5.07297 [28832/165515]
training || loss:0.0003146 MAE:6.37928 [32032/165515]
training || loss:0.0001860 MAE:5.25142 [35232/165515]
training || loss:0.0003085 MAE:5.88601 [38432/165515]
training || loss:0.0003152 MAE:6.12694 [41632/165515]
training || loss:0.0005103 MAE:6.84734 [44832/165515]
training || loss:0.0004215 MAE:5.74464 [48032/165515]
training || loss:0.0001662 MAE:4.05163 [51232/165515]
training || loss:0.0002036 MAE:4.95962 [54432/165515]
training || loss:0.0002584 MAE:5.74832 [57632/165515]
training || loss:0.0003201 MAE:5.81100 [60832/165515]
training || loss:0.0002641 MAE:5.10827 [64032/165515]
training || loss:0.0002270 MAE:5.36908 [67232/165515]
training || loss:0.0002592 MAE:5.69402 [70432/165515]
training || loss:0.0002692 MAE:5.05828 [73632/165515]
training || loss:0.0002210 MAE:5.07638 [76832/165515]
training || loss:0.0001743 MAE:5.06784 [80032/165515]
training || loss:0.0002242 MAE:5.63653 [83232/165515]
training || loss:0.0001978 MAE:4.91164 [86432/165515]
training || loss:0.0002022 MAE:4.40923 [89632/165515]
training || loss:0.0002891 MAE:5.22561 [92832/165515]
training || loss:0.0002222 MAE:4.98214 [96032/165515]
training || loss:0.0001553 MAE:4.43367 [99232/165515]
training || loss:0.0002341 MAE:5.07355 [102432/165515]
training || loss:0.0002999 MAE:5.79954 [105632/165515]
training || loss:0.0002284 MAE:4.79086 [108832/165515]
training || loss:0.0005109 MAE:6.09402 [112032/165515]
training || loss:0.0002401 MAE:5.66996 [115232/165515]
training || loss:0.0002977 MAE:6.25996 [118432/165515]
training || loss:0.0002762 MAE:6.26703 [121632/165515]
training || loss:0.0002691 MAE:6.34967 [124832/165515]
training || loss:0.0002253 MAE:4.98894 [128032/165515]
training || loss:0.0002744 MAE:6.47858 [131232/165515]
training || loss:0.0002053 MAE:4.49872 [134432/165515]
training || loss:0.0002651 MAE:5.63706 [137632/165515]
training || loss:0.0002529 MAE:5.55132 [140832/165515]
training || loss:0.0001823 MAE:4.20850 [144032/165515]
training || loss:0.0002679 MAE:5.85743 [147232/165515]
training || loss:0.0002371 MAE:5.20820 [150432/165515]
training || loss:0.0003449 MAE:6.30683 [153632/165515]
training || loss:0.0002547 MAE:5.72629 [156832/165515]
training || loss:0.0002405 MAE:4.79543 [160032/165515]
training || loss:0.0002076 MAE:4.57701 [163232/165515]
validate|| MAE:2.29875
-----------------------epoch 8-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0001849 MAE:4.51030 [32/165515]
training || loss:0.0002279 MAE:5.84987 [3232/165515]
training || loss:0.0001866 MAE:4.81010 [6432/165515]
training || loss:0.0002756 MAE:6.16298 [9632/165515]
training || loss:0.0002434 MAE:5.64336 [12832/165515]
training || loss:0.0003243 MAE:5.60859 [16032/165515]
training || loss:0.0002285 MAE:5.47969 [19232/165515]
training || loss:0.0003078 MAE:5.28542 [22432/165515]
training || loss:0.0002191 MAE:5.47158 [25632/165515]
training || loss:0.0002219 MAE:5.27160 [28832/165515]
training || loss:0.0003042 MAE:5.99550 [32032/165515]
training || loss:0.0002826 MAE:6.48918 [35232/165515]
training || loss:0.0002436 MAE:5.62755 [38432/165515]
training || loss:0.0003013 MAE:6.89896 [41632/165515]
training || loss:0.0003263 MAE:6.91401 [44832/165515]
training || loss:0.0002283 MAE:4.83024 [48032/165515]
training || loss:0.0002123 MAE:5.37194 [51232/165515]
training || loss:0.0002368 MAE:5.48736 [54432/165515]
training || loss:0.0003896 MAE:6.28565 [57632/165515]
training || loss:0.0002613 MAE:5.91722 [60832/165515]
training || loss:0.0002472 MAE:4.71901 [64032/165515]
training || loss:0.0001450 MAE:4.22315 [67232/165515]
training || loss:0.0002176 MAE:5.56868 [70432/165515]
training || loss:0.0001958 MAE:4.84226 [73632/165515]
training || loss:0.0003093 MAE:6.44677 [76832/165515]
training || loss:0.0001981 MAE:4.70057 [80032/165515]
training || loss:0.0002841 MAE:6.69585 [83232/165515]
training || loss:0.0002851 MAE:6.30977 [86432/165515]
training || loss:0.0002235 MAE:5.04420 [89632/165515]
training || loss:0.0001986 MAE:4.58350 [92832/165515]
training || loss:0.0002686 MAE:5.11958 [96032/165515]
training || loss:0.0001703 MAE:4.39610 [99232/165515]
training || loss:0.0002160 MAE:4.83160 [102432/165515]
training || loss:0.0003144 MAE:6.40823 [105632/165515]
training || loss:0.0002072 MAE:5.23378 [108832/165515]
training || loss:0.0002397 MAE:4.78843 [112032/165515]
training || loss:0.0002268 MAE:5.01241 [115232/165515]
training || loss:0.0002353 MAE:5.42010 [118432/165515]
training || loss:0.0002015 MAE:3.82226 [121632/165515]
training || loss:0.0002593 MAE:5.56481 [124832/165515]
training || loss:0.0002960 MAE:6.50058 [128032/165515]
training || loss:0.0003175 MAE:6.40021 [131232/165515]
training || loss:0.0005101 MAE:6.55978 [134432/165515]
training || loss:0.0001265 MAE:3.82056 [137632/165515]
training || loss:0.0002899 MAE:6.56408 [140832/165515]
training || loss:0.0002333 MAE:4.97978 [144032/165515]
training || loss:0.0001822 MAE:4.48419 [147232/165515]
training || loss:0.0001922 MAE:5.16363 [150432/165515]
training || loss:0.0002513 MAE:5.58080 [153632/165515]
training || loss:0.0001637 MAE:4.60310 [156832/165515]
training || loss:0.0002324 MAE:5.10926 [160032/165515]
training || loss:0.0001903 MAE:4.71788 [163232/165515]
validate|| MAE:2.30104
-----------------------epoch 9-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0002035 MAE:4.13707 [32/165515]
training || loss:0.0002680 MAE:5.91537 [3232/165515]
training || loss:0.0002328 MAE:5.20219 [6432/165515]
training || loss:0.0001919 MAE:5.37349 [9632/165515]
training || loss:0.0002869 MAE:6.11075 [12832/165515]
training || loss:0.0001633 MAE:4.74180 [16032/165515]
training || loss:0.0002036 MAE:4.74109 [19232/165515]
training || loss:0.0002637 MAE:5.94837 [22432/165515]
training || loss:0.0002300 MAE:4.96234 [25632/165515]
training || loss:0.0004085 MAE:6.84967 [28832/165515]
training || loss:0.0001847 MAE:4.14932 [32032/165515]
training || loss:0.0001630 MAE:4.48556 [35232/165515]
training || loss:0.0004559 MAE:7.13641 [38432/165515]
training || loss:0.0001750 MAE:4.45150 [41632/165515]
training || loss:0.0002092 MAE:4.57113 [44832/165515]
training || loss:0.0002255 MAE:5.18661 [48032/165515]
training || loss:0.0001761 MAE:4.47396 [51232/165515]
training || loss:0.0003916 MAE:6.95685 [54432/165515]
training || loss:0.0002777 MAE:5.82587 [57632/165515]
training || loss:0.0002213 MAE:5.23612 [60832/165515]
training || loss:0.0002332 MAE:5.43530 [64032/165515]
training || loss:0.0002948 MAE:6.66206 [67232/165515]
training || loss:0.0004067 MAE:6.17473 [70432/165515]
training || loss:0.0001918 MAE:4.65121 [73632/165515]
training || loss:0.0002447 MAE:4.60619 [76832/165515]
training || loss:0.0002347 MAE:5.83370 [80032/165515]
training || loss:0.0002819 MAE:5.84167 [83232/165515]
training || loss:0.0002596 MAE:5.15514 [86432/165515]
training || loss:0.0002223 MAE:5.80976 [89632/165515]
training || loss:0.0005026 MAE:5.85349 [92832/165515]
training || loss:0.0002374 MAE:5.43084 [96032/165515]
training || loss:0.0003116 MAE:6.03556 [99232/165515]
training || loss:0.0002124 MAE:5.67939 [102432/165515]
training || loss:0.0001930 MAE:4.89918 [105632/165515]
training || loss:0.0001999 MAE:4.47504 [108832/165515]
training || loss:0.0002058 MAE:4.63169 [112032/165515]
training || loss:0.0002825 MAE:5.29952 [115232/165515]
training || loss:0.0001543 MAE:4.56976 [118432/165515]
training || loss:0.0002155 MAE:5.33535 [121632/165515]
training || loss:0.0002150 MAE:5.28683 [124832/165515]
training || loss:0.0002506 MAE:5.13865 [128032/165515]
training || loss:0.0001600 MAE:4.34028 [131232/165515]
training || loss:0.0002175 MAE:5.07177 [134432/165515]
training || loss:0.0004024 MAE:6.83853 [137632/165515]
training || loss:0.0001678 MAE:4.29936 [140832/165515]
training || loss:0.0002124 MAE:4.98181 [144032/165515]
training || loss:0.0002441 MAE:5.24337 [147232/165515]
training || loss:0.0003085 MAE:6.18388 [150432/165515]
training || loss:0.0001730 MAE:4.44243 [153632/165515]
training || loss:0.0002155 MAE:4.93075 [156832/165515]
training || loss:0.0004235 MAE:7.18502 [160032/165515]
training || loss:0.0001665 MAE:3.67470 [163232/165515]
validate|| MAE:2.30288
-----------------------epoch 10-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0002537 MAE:5.57498 [32/165515]
training || loss:0.0002278 MAE:5.14850 [3232/165515]
training || loss:0.0003034 MAE:6.04617 [6432/165515]
training || loss:0.0002443 MAE:5.24094 [9632/165515]
training || loss:0.0002860 MAE:6.18114 [12832/165515]
training || loss:0.0003376 MAE:6.03064 [16032/165515]
training || loss:0.0002161 MAE:5.00776 [19232/165515]
training || loss:0.0002085 MAE:5.05494 [22432/165515]
training || loss:0.0002852 MAE:5.86139 [25632/165515]
training || loss:0.0002609 MAE:4.80795 [28832/165515]
training || loss:0.0001778 MAE:4.82799 [32032/165515]
training || loss:0.0001715 MAE:4.48359 [35232/165515]
training || loss:0.0003328 MAE:7.26060 [38432/165515]
training || loss:0.0001757 MAE:4.67626 [41632/165515]
training || loss:0.0001691 MAE:4.52543 [44832/165515]
training || loss:0.0003222 MAE:5.92269 [48032/165515]
training || loss:0.0001754 MAE:4.33900 [51232/165515]
training || loss:0.0002685 MAE:5.78374 [54432/165515]
training || loss:0.0002388 MAE:5.49111 [57632/165515]
training || loss:0.0002946 MAE:5.91441 [60832/165515]
training || loss:0.0002028 MAE:4.66558 [64032/165515]
training || loss:0.0002147 MAE:4.87766 [67232/165515]
training || loss:0.0001983 MAE:5.11629 [70432/165515]
training || loss:0.0001991 MAE:4.58573 [73632/165515]
training || loss:0.0001854 MAE:4.65761 [76832/165515]
training || loss:0.0004634 MAE:6.69638 [80032/165515]
training || loss:0.0000970 MAE:3.22760 [83232/165515]
training || loss:0.0003339 MAE:5.43331 [86432/165515]
training || loss:0.0003319 MAE:5.63072 [89632/165515]
training || loss:0.0002954 MAE:6.79273 [92832/165515]
training || loss:0.0003396 MAE:6.55543 [96032/165515]
training || loss:0.0001812 MAE:4.58058 [99232/165515]
training || loss:0.0003013 MAE:5.39156 [102432/165515]
training || loss:0.0002003 MAE:5.21605 [105632/165515]
training || loss:0.0001781 MAE:5.16173 [108832/165515]
training || loss:0.0002414 MAE:5.70318 [112032/165515]
training || loss:0.0002201 MAE:5.09020 [115232/165515]
training || loss:0.0002191 MAE:5.16672 [118432/165515]
training || loss:0.0001718 MAE:4.07016 [121632/165515]
training || loss:0.0001875 MAE:4.86687 [124832/165515]
training || loss:0.0002790 MAE:5.59698 [128032/165515]
training || loss:0.0002830 MAE:5.72317 [131232/165515]
training || loss:0.0002149 MAE:5.08752 [134432/165515]
training || loss:0.0002798 MAE:6.07843 [137632/165515]
training || loss:0.0002623 MAE:4.84691 [140832/165515]
training || loss:0.0003778 MAE:7.45456 [144032/165515]
training || loss:0.0002749 MAE:5.63153 [147232/165515]
training || loss:0.0003696 MAE:6.38023 [150432/165515]
training || loss:0.0002284 MAE:4.99761 [153632/165515]
training || loss:0.0001952 MAE:5.19733 [156832/165515]
training || loss:0.0002335 MAE:5.68227 [160032/165515]
training || loss:0.0001909 MAE:4.71933 [163232/165515]
validate|| MAE:2.30315
-----------------------epoch 11-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0002553 MAE:6.10280 [32/165515]
training || loss:0.0004812 MAE:6.05008 [3232/165515]
training || loss:0.0002155 MAE:5.31201 [6432/165515]
training || loss:0.0002674 MAE:6.09445 [9632/165515]
training || loss:0.0002821 MAE:6.03633 [12832/165515]
training || loss:0.0002572 MAE:5.52427 [16032/165515]
training || loss:0.0002834 MAE:5.84062 [19232/165515]
training || loss:0.0002261 MAE:5.29864 [22432/165515]
training || loss:0.0002644 MAE:5.32241 [25632/165515]
training || loss:0.0001837 MAE:4.11695 [28832/165515]
training || loss:0.0001421 MAE:3.90864 [32032/165515]
training || loss:0.0001953 MAE:4.96319 [35232/165515]
training || loss:0.0002156 MAE:4.79983 [38432/165515]
training || loss:0.0001901 MAE:4.66682 [41632/165515]
training || loss:0.0002259 MAE:5.04729 [44832/165515]
training || loss:0.0003201 MAE:5.14809 [48032/165515]
training || loss:0.0003343 MAE:6.23862 [51232/165515]
training || loss:0.0002938 MAE:6.57175 [54432/165515]
training || loss:0.0002491 MAE:5.78188 [57632/165515]
training || loss:0.0002860 MAE:5.62347 [60832/165515]
training || loss:0.0002508 MAE:5.46524 [64032/165515]
training || loss:0.0002691 MAE:5.71551 [67232/165515]
training || loss:0.0005633 MAE:6.92250 [70432/165515]
training || loss:0.0002522 MAE:5.22466 [73632/165515]
training || loss:0.0002019 MAE:5.31592 [76832/165515]
training || loss:0.0003803 MAE:6.06386 [80032/165515]
training || loss:0.0003514 MAE:6.08062 [83232/165515]
training || loss:0.0002448 MAE:5.81060 [86432/165515]
training || loss:0.0006385 MAE:6.73383 [89632/165515]
training || loss:0.0001580 MAE:4.25097 [92832/165515]
training || loss:0.0001902 MAE:4.38201 [96032/165515]
training || loss:0.0001798 MAE:4.55755 [99232/165515]
training || loss:0.0002122 MAE:4.92300 [102432/165515]
training || loss:0.0002738 MAE:5.89320 [105632/165515]
training || loss:0.0001915 MAE:4.41297 [108832/165515]
training || loss:0.0005337 MAE:6.57643 [112032/165515]
training || loss:0.0002027 MAE:4.92823 [115232/165515]
training || loss:0.0002823 MAE:6.00269 [118432/165515]
training || loss:0.0002057 MAE:4.99521 [121632/165515]
training || loss:0.0002172 MAE:5.04886 [124832/165515]
training || loss:0.0005929 MAE:6.63906 [128032/165515]
training || loss:0.0002368 MAE:4.85072 [131232/165515]
training || loss:0.0002377 MAE:5.31705 [134432/165515]
training || loss:0.0002655 MAE:5.58330 [137632/165515]
training || loss:0.0002746 MAE:5.76711 [140832/165515]
training || loss:0.0002148 MAE:5.20022 [144032/165515]
training || loss:0.0002364 MAE:4.93458 [147232/165515]
training || loss:0.0002289 MAE:5.52081 [150432/165515]
training || loss:0.0002079 MAE:4.96320 [153632/165515]
training || loss:0.0002697 MAE:5.74469 [156832/165515]
training || loss:0.0002737 MAE:5.83961 [160032/165515]
training || loss:0.0003361 MAE:7.30463 [163232/165515]
validate|| MAE:2.30364
-----------------------epoch 12-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0002821 MAE:5.80286 [32/165515]
training || loss:0.0002337 MAE:5.05731 [3232/165515]
training || loss:0.0002367 MAE:5.47836 [6432/165515]
training || loss:0.0002948 MAE:6.04515 [9632/165515]
training || loss:0.0003087 MAE:6.51137 [12832/165515]
training || loss:0.0001884 MAE:4.96612 [16032/165515]
training || loss:0.0001917 MAE:4.43886 [19232/165515]
training || loss:0.0003283 MAE:5.18055 [22432/165515]
training || loss:0.0001577 MAE:4.45369 [25632/165515]
training || loss:0.0005574 MAE:6.31129 [28832/165515]
training || loss:0.0002741 MAE:6.21727 [32032/165515]
training || loss:0.0001703 MAE:4.93125 [35232/165515]
training || loss:0.0001948 MAE:5.10469 [38432/165515]
training || loss:0.0002080 MAE:5.02853 [41632/165515]
training || loss:0.0003378 MAE:6.28997 [44832/165515]
training || loss:0.0005243 MAE:6.77164 [48032/165515]
training || loss:0.0002756 MAE:5.57789 [51232/165515]
training || loss:0.0002170 MAE:5.13481 [54432/165515]
training || loss:0.0002207 MAE:4.25735 [57632/165515]
training || loss:0.0004557 MAE:6.25164 [60832/165515]
training || loss:0.0001608 MAE:4.50855 [64032/165515]
training || loss:0.0002349 MAE:5.41632 [67232/165515]
training || loss:0.0002399 MAE:5.20623 [70432/165515]
training || loss:0.0001868 MAE:4.72280 [73632/165515]
training || loss:0.0001991 MAE:5.18415 [76832/165515]
training || loss:0.0001131 MAE:3.86725 [80032/165515]
training || loss:0.0001759 MAE:4.88766 [83232/165515]
training || loss:0.0002658 MAE:5.73138 [86432/165515]
training || loss:0.0002084 MAE:4.94897 [89632/165515]
training || loss:0.0002071 MAE:5.27904 [92832/165515]
training || loss:0.0001747 MAE:5.08628 [96032/165515]
training || loss:0.0002983 MAE:5.58479 [99232/165515]
training || loss:0.0002612 MAE:5.50823 [102432/165515]
training || loss:0.0001857 MAE:4.95488 [105632/165515]
training || loss:0.0002375 MAE:5.33600 [108832/165515]
training || loss:0.0002407 MAE:5.36409 [112032/165515]
training || loss:0.0001665 MAE:3.95030 [115232/165515]
training || loss:0.0002230 MAE:5.18295 [118432/165515]
training || loss:0.0002518 MAE:5.98310 [121632/165515]
training || loss:0.0002870 MAE:5.47974 [124832/165515]
training || loss:0.0001844 MAE:5.03082 [128032/165515]
training || loss:0.0004377 MAE:7.38259 [131232/165515]
training || loss:0.0002277 MAE:5.18803 [134432/165515]
training || loss:0.0002759 MAE:6.32143 [137632/165515]
training || loss:0.0003018 MAE:6.18578 [140832/165515]
training || loss:0.0001310 MAE:4.35281 [144032/165515]
training || loss:0.0002511 MAE:5.07163 [147232/165515]
training || loss:0.0002317 MAE:5.06281 [150432/165515]
training || loss:0.0002316 MAE:5.33930 [153632/165515]
training || loss:0.0002123 MAE:4.81641 [156832/165515]
training || loss:0.0001807 MAE:4.63009 [160032/165515]
training || loss:0.0001404 MAE:4.69838 [163232/165515]
validate|| MAE:2.30395
-----------------------epoch 13-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0002355 MAE:5.54494 [32/165515]
training || loss:0.0002623 MAE:5.72432 [3232/165515]
training || loss:0.0001367 MAE:3.82932 [6432/165515]
training || loss:0.0003044 MAE:6.66450 [9632/165515]
training || loss:0.0002458 MAE:6.05987 [12832/165515]
training || loss:0.0002119 MAE:5.32144 [16032/165515]
training || loss:0.0002569 MAE:5.69482 [19232/165515]
training || loss:0.0001753 MAE:4.81362 [22432/165515]
training || loss:0.0002806 MAE:6.03922 [25632/165515]
training || loss:0.0001934 MAE:4.88898 [28832/165515]
training || loss:0.0001906 MAE:4.47031 [32032/165515]
training || loss:0.0002053 MAE:4.51217 [35232/165515]
training || loss:0.0002293 MAE:5.31179 [38432/165515]
training || loss:0.0001957 MAE:4.89616 [41632/165515]
training || loss:0.0002189 MAE:5.12651 [44832/165515]
training || loss:0.0002020 MAE:5.03907 [48032/165515]
training || loss:0.0002439 MAE:5.26520 [51232/165515]
training || loss:0.0001737 MAE:4.32142 [54432/165515]
training || loss:0.0002281 MAE:5.30861 [57632/165515]
training || loss:0.0002122 MAE:5.30241 [60832/165515]
training || loss:0.0002092 MAE:4.82201 [64032/165515]
training || loss:0.0004680 MAE:6.61085 [67232/165515]
training || loss:0.0002499 MAE:5.65829 [70432/165515]
training || loss:0.0005087 MAE:5.96838 [73632/165515]
training || loss:0.0001830 MAE:4.80867 [76832/165515]
training || loss:0.0002564 MAE:5.16165 [80032/165515]
training || loss:0.0002137 MAE:5.45731 [83232/165515]
training || loss:0.0002348 MAE:5.19448 [86432/165515]
training || loss:0.0002676 MAE:5.86114 [89632/165515]
training || loss:0.0003021 MAE:6.10582 [92832/165515]
training || loss:0.0002558 MAE:5.60253 [96032/165515]
training || loss:0.0002004 MAE:4.45340 [99232/165515]
training || loss:0.0003243 MAE:6.11043 [102432/165515]
training || loss:0.0002357 MAE:5.73741 [105632/165515]
training || loss:0.0002040 MAE:4.81439 [108832/165515]
training || loss:0.0003620 MAE:5.39928 [112032/165515]
training || loss:0.0002385 MAE:5.66858 [115232/165515]
training || loss:0.0001628 MAE:4.35814 [118432/165515]
training || loss:0.0002169 MAE:4.96681 [121632/165515]
training || loss:0.0001466 MAE:4.25571 [124832/165515]
training || loss:0.0003357 MAE:6.15762 [128032/165515]
training || loss:0.0002237 MAE:5.26986 [131232/165515]
training || loss:0.0003611 MAE:5.79632 [134432/165515]
training || loss:0.0001894 MAE:4.86688 [137632/165515]
training || loss:0.0001541 MAE:4.06191 [140832/165515]
training || loss:0.0002575 MAE:5.97026 [144032/165515]
training || loss:0.0002298 MAE:5.09858 [147232/165515]
training || loss:0.0002813 MAE:5.80694 [150432/165515]
training || loss:0.0002049 MAE:5.19337 [153632/165515]
training || loss:0.0003117 MAE:5.95425 [156832/165515]
training || loss:0.0001547 MAE:4.38815 [160032/165515]
training || loss:0.0002101 MAE:5.12608 [163232/165515]
validate|| MAE:2.30526
-----------------------epoch 14-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0001412 MAE:4.52390 [32/165515]
training || loss:0.0002030 MAE:4.73487 [3232/165515]
training || loss:0.0002534 MAE:5.84188 [6432/165515]
training || loss:0.0001714 MAE:4.86154 [9632/165515]
training || loss:0.0001947 MAE:5.43874 [12832/165515]
training || loss:0.0002470 MAE:5.60155 [16032/165515]
training || loss:0.0001913 MAE:4.69183 [19232/165515]
training || loss:0.0002797 MAE:6.22623 [22432/165515]
training || loss:0.0003534 MAE:6.83146 [25632/165515]
training || loss:0.0002080 MAE:4.97789 [28832/165515]
training || loss:0.0005054 MAE:6.09904 [32032/165515]
training || loss:0.0002379 MAE:5.65367 [35232/165515]
training || loss:0.0001772 MAE:5.00848 [38432/165515]
training || loss:0.0002020 MAE:5.08361 [41632/165515]
training || loss:0.0002412 MAE:5.31287 [44832/165515]
training || loss:0.0002449 MAE:5.72710 [48032/165515]
training || loss:0.0001673 MAE:4.02706 [51232/165515]
training || loss:0.0001797 MAE:4.45538 [54432/165515]
training || loss:0.0002303 MAE:5.15928 [57632/165515]
training || loss:0.0002022 MAE:4.58195 [60832/165515]
training || loss:0.0003312 MAE:5.94103 [64032/165515]
training || loss:0.0001993 MAE:5.00516 [67232/165515]
training || loss:0.0002174 MAE:5.54019 [70432/165515]
training || loss:0.0003342 MAE:6.13009 [73632/165515]
training || loss:0.0001887 MAE:5.04953 [76832/165515]
training || loss:0.0002522 MAE:5.93894 [80032/165515]
training || loss:0.0002363 MAE:4.97719 [83232/165515]
training || loss:0.0002154 MAE:4.82322 [86432/165515]
training || loss:0.0002361 MAE:5.34035 [89632/165515]
training || loss:0.0003379 MAE:6.01511 [92832/165515]
training || loss:0.0004954 MAE:6.95081 [96032/165515]
training || loss:0.0002249 MAE:5.25430 [99232/165515]
training || loss:0.0002297 MAE:5.37374 [102432/165515]
training || loss:0.0002622 MAE:5.19950 [105632/165515]
training || loss:0.0002084 MAE:4.92384 [108832/165515]
training || loss:0.0003437 MAE:6.21583 [112032/165515]
training || loss:0.0002765 MAE:5.98611 [115232/165515]
training || loss:0.0001935 MAE:4.90768 [118432/165515]
training || loss:0.0001742 MAE:4.76108 [121632/165515]
training || loss:0.0002407 MAE:5.29231 [124832/165515]
training || loss:0.0002714 MAE:5.95153 [128032/165515]
training || loss:0.0002081 MAE:4.84582 [131232/165515]
training || loss:0.0001834 MAE:5.01688 [134432/165515]
training || loss:0.0002524 MAE:5.46452 [137632/165515]
training || loss:0.0001877 MAE:4.64760 [140832/165515]
training || loss:0.0002860 MAE:5.50914 [144032/165515]
training || loss:0.0003079 MAE:6.43642 [147232/165515]
training || loss:0.0001582 MAE:4.34086 [150432/165515]
training || loss:0.0002315 MAE:5.20746 [153632/165515]
training || loss:0.0002532 MAE:5.80712 [156832/165515]
training || loss:0.0002756 MAE:5.92467 [160032/165515]
training || loss:0.0001833 MAE:5.15295 [163232/165515]
validate|| MAE:2.30467
-----------------------epoch 15-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0001560 MAE:4.94059 [32/165515]
training || loss:0.0002268 MAE:4.83303 [3232/165515]
training || loss:0.0002910 MAE:5.99803 [6432/165515]
training || loss:0.0002174 MAE:4.90430 [9632/165515]
training || loss:0.0002709 MAE:5.60201 [12832/165515]
training || loss:0.0001977 MAE:4.85161 [16032/165515]
training || loss:0.0001689 MAE:4.32314 [19232/165515]
training || loss:0.0002353 MAE:5.42963 [22432/165515]
training || loss:0.0002453 MAE:5.55183 [25632/165515]
training || loss:0.0002111 MAE:4.73457 [28832/165515]
training || loss:0.0001457 MAE:4.35486 [32032/165515]
training || loss:0.0002037 MAE:5.28385 [35232/165515]
training || loss:0.0003116 MAE:6.51151 [38432/165515]
training || loss:0.0001954 MAE:4.96181 [41632/165515]
training || loss:0.0001361 MAE:3.96424 [44832/165515]
training || loss:0.0001941 MAE:4.60062 [48032/165515]
training || loss:0.0002561 MAE:5.90777 [51232/165515]
training || loss:0.0002385 MAE:5.22838 [54432/165515]
training || loss:0.0002474 MAE:5.57482 [57632/165515]
training || loss:0.0001747 MAE:4.47689 [60832/165515]
training || loss:0.0002436 MAE:5.54816 [64032/165515]
training || loss:0.0002643 MAE:5.90227 [67232/165515]
training || loss:0.0001833 MAE:4.54954 [70432/165515]
training || loss:0.0003021 MAE:6.20974 [73632/165515]
training || loss:0.0005794 MAE:6.82695 [76832/165515]
training || loss:0.0002699 MAE:6.07710 [80032/165515]
training || loss:0.0001865 MAE:5.00672 [83232/165515]
training || loss:0.0002019 MAE:4.82541 [86432/165515]
training || loss:0.0001670 MAE:4.48190 [89632/165515]
training || loss:0.0002234 MAE:5.66667 [92832/165515]
training || loss:0.0002761 MAE:5.27157 [96032/165515]
training || loss:0.0001981 MAE:4.82713 [99232/165515]
training || loss:0.0001682 MAE:4.46260 [102432/165515]
training || loss:0.0001141 MAE:3.90715 [105632/165515]
training || loss:0.0003747 MAE:6.33427 [108832/165515]
training || loss:0.0002449 MAE:4.57206 [112032/165515]
training || loss:0.0002560 MAE:5.92988 [115232/165515]
training || loss:0.0004846 MAE:6.51924 [118432/165515]
training || loss:0.0002601 MAE:5.82669 [121632/165515]
training || loss:0.0002233 MAE:4.59466 [124832/165515]
training || loss:0.0002890 MAE:5.47813 [128032/165515]
training || loss:0.0004671 MAE:5.76534 [131232/165515]
training || loss:0.0002101 MAE:4.59308 [134432/165515]
training || loss:0.0002413 MAE:5.77191 [137632/165515]
training || loss:0.0002547 MAE:5.44594 [140832/165515]
training || loss:0.0002464 MAE:5.55377 [144032/165515]
training || loss:0.0001909 MAE:4.26338 [147232/165515]
training || loss:0.0002403 MAE:5.56552 [150432/165515]
training || loss:0.0002290 MAE:5.13935 [153632/165515]
training || loss:0.0001855 MAE:4.51206 [156832/165515]
training || loss:0.0002471 MAE:4.99033 [160032/165515]
training || loss:0.0002055 MAE:4.54270 [163232/165515]
validate|| MAE:2.30646
-----------------------epoch 16-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0002172 MAE:5.33300 [32/165515]
training || loss:0.0001828 MAE:4.91540 [3232/165515]
training || loss:0.0001671 MAE:4.54972 [6432/165515]
training || loss:0.0001616 MAE:4.64133 [9632/165515]
training || loss:0.0002566 MAE:5.77741 [12832/165515]
training || loss:0.0003556 MAE:6.68346 [16032/165515]
training || loss:0.0001884 MAE:4.93516 [19232/165515]
training || loss:0.0001940 MAE:5.24225 [22432/165515]
training || loss:0.0003357 MAE:7.19005 [25632/165515]
training || loss:0.0001925 MAE:5.06560 [28832/165515]
training || loss:0.0003359 MAE:5.96066 [32032/165515]
training || loss:0.0002490 MAE:5.30153 [35232/165515]
training || loss:0.0002242 MAE:5.44274 [38432/165515]
training || loss:0.0001798 MAE:4.70997 [41632/165515]
training || loss:0.0002442 MAE:5.49288 [44832/165515]
training || loss:0.0002180 MAE:5.26686 [48032/165515]
training || loss:0.0003118 MAE:6.11432 [51232/165515]
training || loss:0.0002581 MAE:5.85657 [54432/165515]
training || loss:0.0002415 MAE:5.27590 [57632/165515]
training || loss:0.0001848 MAE:5.04326 [60832/165515]
training || loss:0.0002207 MAE:5.42356 [64032/165515]
training || loss:0.0001553 MAE:4.50307 [67232/165515]
training || loss:0.0001580 MAE:4.72618 [70432/165515]
training || loss:0.0002935 MAE:6.08696 [73632/165515]
training || loss:0.0002386 MAE:5.25794 [76832/165515]
training || loss:0.0001475 MAE:4.50299 [80032/165515]
training || loss:0.0002593 MAE:5.95265 [83232/165515]
training || loss:0.0003990 MAE:5.46956 [86432/165515]
training || loss:0.0002155 MAE:4.92680 [89632/165515]
training || loss:0.0002350 MAE:5.24177 [92832/165515]
training || loss:0.0002127 MAE:4.89629 [96032/165515]
training || loss:0.0001184 MAE:3.83981 [99232/165515]
training || loss:0.0002623 MAE:5.91923 [102432/165515]
training || loss:0.0001857 MAE:4.34807 [105632/165515]
training || loss:0.0001853 MAE:5.14210 [108832/165515]
training || loss:0.0002165 MAE:5.12456 [112032/165515]
training || loss:0.0001865 MAE:4.84482 [115232/165515]
training || loss:0.0001883 MAE:4.97132 [118432/165515]
training || loss:0.0001894 MAE:4.90635 [121632/165515]
training || loss:0.0002589 MAE:5.85579 [124832/165515]
training || loss:0.0002928 MAE:5.97927 [128032/165515]
training || loss:0.0002592 MAE:5.43111 [131232/165515]
training || loss:0.0002191 MAE:4.72346 [134432/165515]
training || loss:0.0004771 MAE:6.70495 [137632/165515]
training || loss:0.0002445 MAE:5.26975 [140832/165515]
training || loss:0.0007229 MAE:7.53479 [144032/165515]
training || loss:0.0001453 MAE:3.81034 [147232/165515]
training || loss:0.0002321 MAE:5.42930 [150432/165515]
training || loss:0.0001910 MAE:4.53160 [153632/165515]
training || loss:0.0003394 MAE:5.64090 [156832/165515]
training || loss:0.0002052 MAE:4.78435 [160032/165515]
training || loss:0.0002254 MAE:5.88069 [163232/165515]
validate|| MAE:2.30582
-----------------------epoch 17-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0001859 MAE:5.03106 [32/165515]
training || loss:0.0001827 MAE:4.75716 [3232/165515]
training || loss:0.0001949 MAE:5.27517 [6432/165515]
training || loss:0.0001370 MAE:3.78279 [9632/165515]
training || loss:0.0001847 MAE:4.77746 [12832/165515]
training || loss:0.0002552 MAE:5.78794 [16032/165515]
training || loss:0.0002526 MAE:5.78279 [19232/165515]
training || loss:0.0002144 MAE:4.84228 [22432/165515]
training || loss:0.0001800 MAE:4.30297 [25632/165515]
training || loss:0.0003060 MAE:5.83900 [28832/165515]
training || loss:0.0001940 MAE:5.06092 [32032/165515]
training || loss:0.0004392 MAE:5.81219 [35232/165515]
training || loss:0.0002254 MAE:5.43280 [38432/165515]
training || loss:0.0002124 MAE:4.91023 [41632/165515]
training || loss:0.0002122 MAE:4.94892 [44832/165515]
training || loss:0.0002928 MAE:6.01359 [48032/165515]
training || loss:0.0001347 MAE:3.80052 [51232/165515]
training || loss:0.0002727 MAE:5.62381 [54432/165515]
training || loss:0.0001284 MAE:3.99990 [57632/165515]
training || loss:0.0001859 MAE:4.67034 [60832/165515]
training || loss:0.0002893 MAE:5.93716 [64032/165515]
training || loss:0.0002375 MAE:5.06473 [67232/165515]
training || loss:0.0001962 MAE:5.04559 [70432/165515]
training || loss:0.0002207 MAE:4.86907 [73632/165515]
training || loss:0.0002948 MAE:6.66374 [76832/165515]
training || loss:0.0002030 MAE:5.01062 [80032/165515]
training || loss:0.0001903 MAE:4.75912 [83232/165515]
training || loss:0.0001797 MAE:4.41386 [86432/165515]
training || loss:0.0002161 MAE:4.73539 [89632/165515]
training || loss:0.0001598 MAE:4.53971 [92832/165515]
training || loss:0.0001976 MAE:5.04040 [96032/165515]
training || loss:0.0002551 MAE:4.90726 [99232/165515]
training || loss:0.0002721 MAE:5.89976 [102432/165515]
training || loss:0.0002157 MAE:4.92831 [105632/165515]
training || loss:0.0002836 MAE:5.61116 [108832/165515]
training || loss:0.0001400 MAE:3.91102 [112032/165515]
training || loss:0.0001937 MAE:5.02226 [115232/165515]
training || loss:0.0001497 MAE:4.04381 [118432/165515]
training || loss:0.0001528 MAE:4.06925 [121632/165515]
training || loss:0.0002384 MAE:5.68581 [124832/165515]
training || loss:0.0002711 MAE:6.14925 [128032/165515]
training || loss:0.0001724 MAE:4.34331 [131232/165515]
training || loss:0.0003088 MAE:5.71031 [134432/165515]
training || loss:0.0002930 MAE:5.58022 [137632/165515]
training || loss:0.0002441 MAE:5.21487 [140832/165515]
training || loss:0.0002280 MAE:4.43012 [144032/165515]
training || loss:0.0002526 MAE:5.75728 [147232/165515]
training || loss:0.0002200 MAE:5.61853 [150432/165515]
training || loss:0.0002652 MAE:5.73464 [153632/165515]
training || loss:0.0003657 MAE:6.57471 [156832/165515]
training || loss:0.0001876 MAE:5.18095 [160032/165515]
training || loss:0.0002084 MAE:4.92369 [163232/165515]
validate|| MAE:2.30655
-----------------------epoch 18-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0001850 MAE:4.81953 [32/165515]
training || loss:0.0002685 MAE:5.97116 [3232/165515]
training || loss:0.0002321 MAE:5.54968 [6432/165515]
training || loss:0.0001893 MAE:4.80049 [9632/165515]
training || loss:0.0001829 MAE:4.33571 [12832/165515]
training || loss:0.0002622 MAE:5.15323 [16032/165515]
training || loss:0.0002544 MAE:6.08920 [19232/165515]
training || loss:0.0001967 MAE:4.86712 [22432/165515]
training || loss:0.0001609 MAE:4.70570 [25632/165515]
training || loss:0.0002922 MAE:6.17799 [28832/165515]
training || loss:0.0002881 MAE:5.74258 [32032/165515]
training || loss:0.0002215 MAE:5.12283 [35232/165515]
training || loss:0.0002284 MAE:5.17371 [38432/165515]
training || loss:0.0002312 MAE:5.28593 [41632/165515]
training || loss:0.0002028 MAE:4.14186 [44832/165515]
training || loss:0.0002156 MAE:5.09129 [48032/165515]
training || loss:0.0002403 MAE:5.52948 [51232/165515]
training || loss:0.0002210 MAE:5.19975 [54432/165515]
training || loss:0.0001171 MAE:3.53376 [57632/165515]
training || loss:0.0001557 MAE:4.25017 [60832/165515]
training || loss:0.0003021 MAE:6.27307 [64032/165515]
training || loss:0.0002485 MAE:5.54942 [67232/165515]
training || loss:0.0002914 MAE:6.04686 [70432/165515]
training || loss:0.0003323 MAE:6.22030 [73632/165515]
training || loss:0.0002917 MAE:5.96313 [76832/165515]
training || loss:0.0001879 MAE:4.92953 [80032/165515]
training || loss:0.0001403 MAE:3.94600 [83232/165515]
training || loss:0.0001948 MAE:4.69357 [86432/165515]
training || loss:0.0001406 MAE:4.38410 [89632/165515]
training || loss:0.0002885 MAE:5.91883 [92832/165515]
training || loss:0.0002105 MAE:5.01846 [96032/165515]
training || loss:0.0001005 MAE:3.65320 [99232/165515]
training || loss:0.0002039 MAE:4.75028 [102432/165515]
training || loss:0.0002505 MAE:5.42787 [105632/165515]
training || loss:0.0002869 MAE:5.79967 [108832/165515]
training || loss:0.0002049 MAE:4.70965 [112032/165515]
training || loss:0.0003710 MAE:6.63791 [115232/165515]
training || loss:0.0005080 MAE:5.15012 [118432/165515]
training || loss:0.0002380 MAE:5.65246 [121632/165515]
training || loss:0.0002463 MAE:5.28984 [124832/165515]
training || loss:0.0001904 MAE:5.10872 [128032/165515]
training || loss:0.0002948 MAE:6.46947 [131232/165515]
training || loss:0.0002060 MAE:4.79078 [134432/165515]
training || loss:0.0001633 MAE:4.62674 [137632/165515]
training || loss:0.0003830 MAE:5.64970 [140832/165515]
training || loss:0.0002596 MAE:5.82100 [144032/165515]
training || loss:0.0002595 MAE:5.36844 [147232/165515]
training || loss:0.0002593 MAE:5.76771 [150432/165515]
training || loss:0.0002503 MAE:5.84633 [153632/165515]
training || loss:0.0001787 MAE:5.00312 [156832/165515]
training || loss:0.0001638 MAE:4.36186 [160032/165515]
training || loss:0.0002524 MAE:5.64849 [163232/165515]
validate|| MAE:2.30698
-----------------------epoch 19-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0001920 MAE:5.23997 [32/165515]
training || loss:0.0002798 MAE:5.82571 [3232/165515]
training || loss:0.0002051 MAE:5.33839 [6432/165515]
training || loss:0.0001916 MAE:5.33343 [9632/165515]
training || loss:0.0002023 MAE:5.22126 [12832/165515]
training || loss:0.0001843 MAE:4.93863 [16032/165515]
training || loss:0.0002621 MAE:6.10435 [19232/165515]
training || loss:0.0001990 MAE:4.50691 [22432/165515]
training || loss:0.0001583 MAE:4.56885 [25632/165515]
training || loss:0.0002828 MAE:6.07117 [28832/165515]
training || loss:0.0002634 MAE:5.79434 [32032/165515]
training || loss:0.0001631 MAE:4.48640 [35232/165515]
training || loss:0.0002527 MAE:5.36627 [38432/165515]
training || loss:0.0003273 MAE:5.67766 [41632/165515]
training || loss:0.0002536 MAE:4.99348 [44832/165515]
training || loss:0.0001342 MAE:4.22428 [48032/165515]
training || loss:0.0001945 MAE:4.83541 [51232/165515]
training || loss:0.0001960 MAE:5.11384 [54432/165515]
training || loss:0.0001803 MAE:4.35494 [57632/165515]
training || loss:0.0001676 MAE:4.73077 [60832/165515]
training || loss:0.0002325 MAE:5.43512 [64032/165515]
training || loss:0.0001830 MAE:4.83074 [67232/165515]
training || loss:0.0001990 MAE:4.91567 [70432/165515]
training || loss:0.0002185 MAE:5.25313 [73632/165515]
training || loss:0.0003553 MAE:6.08033 [76832/165515]
training || loss:0.0002567 MAE:5.75064 [80032/165515]
training || loss:0.0002362 MAE:5.51828 [83232/165515]
training || loss:0.0003022 MAE:6.26179 [86432/165515]
training || loss:0.0003204 MAE:6.26856 [89632/165515]
training || loss:0.0002081 MAE:4.89972 [92832/165515]
training || loss:0.0001630 MAE:4.40856 [96032/165515]
training || loss:0.0002277 MAE:5.26577 [99232/165515]
training || loss:0.0001847 MAE:5.03171 [102432/165515]
training || loss:0.0002121 MAE:5.30201 [105632/165515]
training || loss:0.0004317 MAE:5.62477 [108832/165515]
training || loss:0.0001597 MAE:4.52906 [112032/165515]
training || loss:0.0001766 MAE:4.40967 [115232/165515]
training || loss:0.0001926 MAE:4.89532 [118432/165515]
training || loss:0.0003079 MAE:5.07633 [121632/165515]
training || loss:0.0002230 MAE:5.45594 [124832/165515]
training || loss:0.0002166 MAE:4.28903 [128032/165515]
training || loss:0.0001558 MAE:4.18912 [131232/165515]
training || loss:0.0002416 MAE:5.38230 [134432/165515]
training || loss:0.0003176 MAE:5.39135 [137632/165515]
training || loss:0.0002525 MAE:5.24328 [140832/165515]
training || loss:0.0001814 MAE:4.49716 [144032/165515]
training || loss:0.0002074 MAE:5.00558 [147232/165515]
training || loss:0.0002225 MAE:5.49406 [150432/165515]
training || loss:0.0001676 MAE:4.82178 [153632/165515]
training || loss:0.0001832 MAE:5.02036 [156832/165515]
training || loss:0.0001384 MAE:4.17105 [160032/165515]
training || loss:0.0001429 MAE:4.02696 [163232/165515]
validate|| MAE:2.30636
-----------------------epoch 20-----------------------
-----------current learning rate: 0.001000-----------
training || loss:0.0002442 MAE:5.39025 [32/165515]
training || loss:0.0003118 MAE:5.79610 [3232/165515]
training || loss:0.0001994 MAE:4.75524 [6432/165515]
training || loss:0.0001903 MAE:4.69471 [9632/165515]
training || loss:0.0001860 MAE:4.66508 [12832/165515]
training || loss:0.0002677 MAE:5.54792 [16032/165515]
training || loss:0.0001665 MAE:4.68207 [19232/165515]
training || loss:0.0003068 MAE:5.40713 [22432/165515]
training || loss:0.0002046 MAE:4.83120 [25632/165515]
training || loss:0.0002431 MAE:5.80487 [28832/165515]
training || loss:0.0002929 MAE:6.01527 [32032/165515]
training || loss:0.0002296 MAE:5.57111 [35232/165515]
training || loss:0.0002477 MAE:6.07600 [38432/165515]
training || loss:0.0002623 MAE:5.87583 [41632/165515]
training || loss:0.0002764 MAE:5.87066 [44832/165515]
training || loss:0.0002379 MAE:5.82781 [48032/165515]
training || loss:0.0002171 MAE:4.97158 [51232/165515]
training || loss:0.0001728 MAE:4.60934 [54432/165515]
training || loss:0.0001581 MAE:4.41835 [57632/165515]
training || loss:0.0002257 MAE:5.38183 [60832/165515]
training || loss:0.0003795 MAE:6.20409 [64032/165515]
training || loss:0.0002720 MAE:5.64667 [67232/165515]
training || loss:0.0002638 MAE:6.07769 [70432/165515]
training || loss:0.0002174 MAE:5.55314 [73632/165515]
training || loss:0.0002378 MAE:5.29885 [76832/165515]
training || loss:0.0002607 MAE:6.13111 [80032/165515]
training || loss:0.0003006 MAE:6.50329 [83232/165515]
training || loss:0.0002309 MAE:6.09483 [86432/165515]
training || loss:0.0001745 MAE:4.57421 [89632/165515]
training || loss:0.0002234 MAE:5.41909 [92832/165515]
training || loss:0.0002097 MAE:4.92422 [96032/165515]
training || loss:0.0002107 MAE:5.38093 [99232/165515]
training || loss:0.0002182 MAE:5.42060 [102432/165515]
training || loss:0.0002561 MAE:5.86117 [105632/165515]
training || loss:0.0001389 MAE:4.14339 [108832/165515]
training || loss:0.0002503 MAE:5.44598 [112032/165515]
training || loss:0.0003012 MAE:6.24997 [115232/165515]
training || loss:0.0001385 MAE:4.12195 [118432/165515]
training || loss:0.0002139 MAE:5.05098 [121632/165515]
training || loss:0.0002012 MAE:4.90193 [124832/165515]
training || loss:0.0002336 MAE:5.33424 [128032/165515]
training || loss:0.0001680 MAE:4.63467 [131232/165515]
training || loss:0.0002785 MAE:6.29745 [134432/165515]
training || loss:0.0002974 MAE:5.83200 [137632/165515]
training || loss:0.0001476 MAE:4.11353 [140832/165515]
training || loss:0.0005551 MAE:6.24162 [144032/165515]
training || loss:0.0001948 MAE:4.80501 [147232/165515]
training || loss:0.0001860 MAE:4.78044 [150432/165515]
training || loss:0.0001730 MAE:4.89044 [153632/165515]
training || loss:0.0001713 MAE:4.64428 [156832/165515]
training || loss:0.0002849 MAE:6.41321 [160032/165515]
training || loss:0.0002033 MAE:4.94201 [163232/165515]
validate|| MAE:2.30730


PS:

Read file </nfsshare/home/xiechenghan/DRO_trustregion/log/NAME_255427.err> for stderr output of this job.

